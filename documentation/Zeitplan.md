## Zeitplan Stand 22.04.2022

### KW 13-14:
- Einlesen in Basics von RL 
- Abklärung Thema 
- Training erster Modelle 
- Einrichtung Programme 

### KW 15-16:
- Einlesen Rewards 
- Weitere Einarbeitung in RL 


### KW 17-18:
- Erstellung eines Scenarios in Carla 
- Implementierung RL mit distance Auto - Person 

### KW 19-20:
- Erweiterung von Modellen und überlegung Erweiterung Action/ obs space 
- Sinnvoller Kollision-Reward finden

### KW 21-22:
- RTX 3080TI zum Laufen bringen


### KW 23
- Env überprüfen
- Determinismus überprüfen
- Reward verbessern 
- Obs Space erweitern 
- (Action Space)


### KW 24
- Initialisierung high low/ Eigenschaften Box Bounds 
- Action Sequenz speichern
- Dokumentation 


### KW 25-27
- Optuna versuchen und effizienter zu werden 
- Add Render Mode(s)  
- Learning Rate/ Parameter anpassen 
- Welche Epochen anzahl ist sinnvoll?
- Add Force/ set_target_velocity
- Async Mode testen
- Ergebnisse Determinismus zusammenschreiben
- Eigenschaften Model betrachten
- 

### KW 28-29
- Optuna mit mehr parametern
- Zurück zum Eigentlichen Problem ... -> Klassifizierung finden und situation bewerten
- Größeres Model?! 
- Problem mit Environment Crash behoben 
- Sehr viel Training 


### KW 30 
- Optuna Multi-Rechner setup 
- parameterraum vergrößern
- erste Auswertung was gute Parameter sein können  
- überlegung wie schnell soll fahrzeug sein -> 30kmh zum start

### KW 31 
- Reward-Funktion (nicht richtung Auto schauen)
- Startposition Gehweg
- Action Space erweitern (v walker)
- Observation Space (Orientation Walker) 
- Optuna Training starten 
- Optuna Doku
- Parameter verstehen 

<hr>

### KW 32 
- Reward-Funktion anpassen 
- Doku installation + optuna + schicken
- Issue Github Determinism

### KW 33 
- Visualisierung Rewards
- Einfache Implementierung zum Testen 
- Bestandteile Reward im Info Feld
- Reward-Design
- Einleitung Abschlusspräsentation

### WK 34 + 
- Dokumentation 
- Präsentation
- Paper lesen 
- Freies Arbeiten 

