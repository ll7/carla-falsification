## Zeitplan Stand 22.04.2022

### KW 13-14:
- Einlesen in Basics von RL 
- Abklaerung Thema 
- Training erster Modelle 
- Einrichtung Programme 

### KW 15-16:
- Einlesen Rewards 
- Weitere Einarbeitung in RL 


### KW 17-18:
- Erstellung eines Scenarios in Carla 
- Implementierung RL mit distance Auto - Person 

### KW 19-20:
- Erweiterung von Modellen und ueberlegung Erweiterung Action/ obs space 
- Sinnvoller Kollisionsreward finden

### KW 21-22:
- RTX 3080TI zum laufen bringen


### KW 23
- Env 端berpr端fen
- Determinismus 端berpr端fen
- Reward verbessern 
- Obs Space erweitern 
- (Action Space)

<hr>

### KW 24
- Initialisisung high low/ Eigenschaften Box Bounds 
- Action Sequenz speichern 
<br>

- Eigentschaften Model betrachten 
- (Ergebnisse Determinismus zusammenschreiben/ Issue Github)
- Dokumentation 


### KW 25+
- Optuna versuchen und effizienter zu werden
- Learning Rate/ Parameter anpassen 
- Welche Epochen anzahl ist sinnvoll? 
- Trainingsverfahren
- Andere Policy 
- Add Force/ set_target_velocity
- Weitere Scenarien hinzufuegen 
- Async Mode testen
- https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe


### ToDo 
- Abschlusspraesentation erstellen
- Doku
- Abschlusspraesentation
