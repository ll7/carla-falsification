## Zeitplan Stand 22.04.2022

### KW 13-14:
- Einlesen in Basics von RL 
- Abklaerung Thema 
- Training erster Modelle 
- Einrichtung Programme 

### KW 15-16:
- Einlesen Rewards 
- Weitere Einarbeitung in RL 


### KW 17-18:
- Erstellung eines Scenarios in Carla 
- Implementierung RL mit distance Auto - Person 

### KW 19-20:
- Erweiterung von Modellen und ueberlegung Erweiterung Action/ obs space 
- Sinnvoller Kollisionsreward finden

### KW 21-22:
- RTX 3080TI zum laufen bringen


### KW 23
- Env überprüfen
- Determinismus überprüfen
- Reward verbessern 
- Obs Space erweitern 
- (Action Space)


### KW 24
- Initialisisung high low/ Eigenschaften Box Bounds 
- Action Sequenz speichern
- Dokumentation 


### KW 25-27
- Optuna versuchen und effizienter zu werden 
- Add Render Mode(s)  
- Learning Rate/ Parameter anpassen 
- Welche Epochen anzahl ist sinnvoll?
- Add Force/ set_target_velocity
- Async Mode testen
- Ergebnisse Determinismus zusammenschreiben
- Eigentschaften Model betrachten
- 

### KW 28-29
- Optuna mit mehr parametern
- Zurück zum Eigentlichen Problem ... -> Klassifizierung finden und situation bewerten
- Größeres Model?! 
- Problem mit Envirionment Crash behoben 
- Sehr viel Training 
<hr>

### KW 30 
- Trainingsverfahren testen (PPO, A2C, DDPG, SAC, TD3)
- Issue Github Determinism
- Weitere Scenarien hinzufuegen (Interface zum schnellen Einfügen)
- Force vector in Abhänigkeit wie das Fahrzeug steht + überlegung wie schnell soll fahrzeug sein

### KW 36 
- Anmeldung Masterarbeit!

### ToDo 
- Abschlusspräsentation erstellen
- Doku
